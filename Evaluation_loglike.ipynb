{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Please run this notebook on Google Collab to not have dependencies issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "14aR_W44Xf9N",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### GPT-2 Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "boVrVrLEUrhg",
    "outputId": "a92a8fa7-8444-49b8-c776-d8ee21073419"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/data/datasets/language_modeling.py:119: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3750' max='3750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3750/3750 3:56:58, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.032400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.594400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.521300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.297500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.419700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.365700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.200700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.271000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.247800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.057000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.047400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.159200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.899300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.941600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.832800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.834400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.879200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.818400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.908100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.789400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.774400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.851300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.887800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.779000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.747400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.668000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.712200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.627500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.703600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.626500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.633100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.631100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.691100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.661700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.603200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.633700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.639800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Training complete. Model saved!\n",
      "\n",
      "📘 Generated Physics Question:\n",
      "\n",
      "[TOPIC: Electrostatics] [GRADE: 12] [LEVEL: Advanced] [DIFFICULTY: 7]\n",
      "Q: Calculate the electrostatic force between two point charges using Coulomb's Law, derived from the superposition of forces due to charges. [TYPE: Proof] [COMPLEXITY: 1.2] [COMPLEXITY: 1.1] [COMPLEXITY: 3.1] [COMPLEXITY: 2.8]\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers --quiet\n",
    "\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "from transformers import (\n",
    "    GPT2Tokenizer, GPT2LMHeadModel,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    LineByLineTextDataset,\n",
    "    Trainer, TrainingArguments\n",
    ")\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"/content/physics_questions_500.csv\")\n",
    "df.fillna(\"N/A\", inplace=True)\n",
    "\n",
    "formatted_data = []\n",
    "for _, row in df.iterrows():\n",
    "    block = f\"\"\"[TOPIC: {row['Topic']}] [GRADE: {row['grade']}] [LEVEL: {row['StudentLevel']}] [DIFFICULTY: {row['Difficulty']}] [TYPE: {row['QuestionType']}] [COMPLEXITY: {row['QuestionComplexity']}]\n",
    "[PREREQUISITES: {row['Prerequisites']}] [ESTIMATED_TIME: {row['EstimatedTime']}] [SUBJECT: {row['subject']}]\n",
    "\n",
    "Q: {row['Question']}\n",
    "A: {row['Answer']}\n",
    "\n",
    "[EXPLANATION: {row['Explanation']}]\n",
    "\"\"\"\n",
    "    formatted_data.append(block)\n",
    "\n",
    "with open(\"formatted_physics.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\\n\".join(formatted_data))\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "dataset = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"formatted_physics.txt\",\n",
    "    block_size=128\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-physics\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    save_steps=250,\n",
    "    save_total_limit=1,\n",
    "    logging_steps=100,\n",
    "    prediction_loss_only=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model(\"./gpt2-physics\")\n",
    "tokenizer.save_pretrained(\"./gpt2-physics\")\n",
    "print(\"Training complete. Model saved!\")\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"./gpt2-physics\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"./gpt2-physics\")\n",
    "model.eval()\n",
    "\n",
    "prompt = \"[TOPIC: Electrostatics] [GRADE: 12] [LEVEL: Advanced] [DIFFICULTY: 7]\\nQ:\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "\n",
    "generated = model.generate(\n",
    "    input_ids,\n",
    "    max_length=100,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    temperature=0.8,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "print(\"\\nGenerated Physics Question:\\n\")\n",
    "print(tokenizer.decode(generated[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upt2WM28XkxF"
   },
   "source": [
    "Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 486
    },
    "id": "IL6tOPJpPEiC",
    "outputId": "43ba0606-1328-47f3-f115-1af334de9be9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: gpt2-physics/ (stored 0%)\n",
      "  adding: gpt2-physics/model.safetensors (deflated 7%)\n",
      "  adding: gpt2-physics/vocab.json (deflated 68%)\n",
      "  adding: gpt2-physics/config.json (deflated 51%)\n",
      "  adding: gpt2-physics/tokenizer_config.json (deflated 56%)\n",
      "  adding: gpt2-physics/runs/ (stored 0%)\n",
      "  adding: gpt2-physics/runs/Apr12_09-28-08_e45f0af0f25d/ (stored 0%)\n",
      "  adding: gpt2-physics/runs/Apr12_09-28-08_e45f0af0f25d/events.out.tfevents.1744450089.e45f0af0f25d.240.1 (deflated 64%)\n",
      "  adding: gpt2-physics/runs/Apr12_09-23-03_e45f0af0f25d/ (stored 0%)\n",
      "  adding: gpt2-physics/runs/Apr12_09-23-03_e45f0af0f25d/events.out.tfevents.1744449787.e45f0af0f25d.240.0 (deflated 61%)\n",
      "  adding: gpt2-physics/generation_config.json (deflated 24%)\n",
      "  adding: gpt2-physics/checkpoint-3750/ (stored 0%)\n",
      "  adding: gpt2-physics/checkpoint-3750/model.safetensors (deflated 7%)\n",
      "  adding: gpt2-physics/checkpoint-3750/vocab.json (deflated 68%)\n",
      "  adding: gpt2-physics/checkpoint-3750/config.json (deflated 51%)\n",
      "  adding: gpt2-physics/checkpoint-3750/tokenizer_config.json (deflated 56%)\n",
      "  adding: gpt2-physics/checkpoint-3750/scheduler.pt (deflated 56%)\n",
      "  adding: gpt2-physics/checkpoint-3750/rng_state.pth (deflated 24%)\n",
      "  adding: gpt2-physics/checkpoint-3750/generation_config.json (deflated 24%)\n",
      "  adding: gpt2-physics/checkpoint-3750/optimizer.pt (deflated 8%)\n",
      "  adding: gpt2-physics/checkpoint-3750/merges.txt (deflated 53%)\n",
      "  adding: gpt2-physics/checkpoint-3750/training_args.bin (deflated 52%)\n",
      "  adding: gpt2-physics/checkpoint-3750/special_tokens_map.json (deflated 74%)\n",
      "  adding: gpt2-physics/checkpoint-3750/trainer_state.json (deflated 78%)\n",
      "  adding: gpt2-physics/merges.txt (deflated 53%)\n",
      "  adding: gpt2-physics/training_args.bin (deflated 52%)\n",
      "  adding: gpt2-physics/special_tokens_map.json (deflated 74%)\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_3b1a210d-daf6-447e-8897-533f708da91f\", \"gpt2-physics.zip\", 1841076737)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!zip -r gpt2-physics.zip ./gpt2-physics\n",
    "from google.colab import files\n",
    "files.download(\"gpt2-physics.zip\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QZOAbxu5XpDd"
   },
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of Gemini and GPT-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### GPt-2 Generation and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9niVzCUbS3Zb",
    "outputId": "a2e1bdfc-2d45-46fa-9f42-57a4f721b7da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📌 Input Text: Q: What is the unit of electric charge? A: Coulomb.\n",
      "🔹 Token Count: 15\n",
      "🔹 Cross-Entropy Loss: 2.0931\n",
      "🔹 Total Log-Likelihood: -31.3962\n",
      "🔹 Perplexity: 8.1099\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"./gpt2-physics\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"./gpt2-physics\")\n",
    "model.eval()\n",
    "\n",
    "def get_loss_and_perplexity(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt')\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, labels=input_ids)\n",
    "        loss = outputs.loss\n",
    "        total_log_likelihood = -loss.item() * input_ids.shape[1]\n",
    "        perplexity = torch.exp(loss)\n",
    "\n",
    "    print(f\"\\n Input Text: {text}\")\n",
    "    print(f\"🔹 Token Count: {input_ids.shape[1]}\")\n",
    "    print(f\"🔹 Cross-Entropy Loss: {loss.item():.4f}\")\n",
    "    print(f\"🔹 Total Log-Likelihood: {total_log_likelihood:.4f}\")\n",
    "    print(f\"🔹 Perplexity: {perplexity.item():.4f}\")\n",
    "\n",
    "get_loss_and_perplexity(\"Q: What is the unit of electric charge? A: Coulomb.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-wcXREokXMV5",
    "outputId": "ab346c2c-71ae-4c73-eeea-cf52f3f13e9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Generated Question:\n",
      " [TOPIC: Electrostatics] [GRADE: 12] [LEVEL: Intermediate] [DIFFICULTY: 5]\n",
      "Q: What is the electric field at a point P due to two charges q1 and q2? [PERC: N/A] [SUBJECT: Physics] [TYPE: General] [COMPLEXITY: 2.6] [COMPLEXITY: 2.1] [COMPLEXITY: 1.4] [COMPLE\n",
      "\n",
      "📊 Evaluation Metrics:\n",
      "🔹 Token Count: 100\n",
      "🔹 Cross-Entropy Loss: 0.9971\n",
      "🔹 Total Log-Likelihood: -99.7149\n",
      "🔹 Perplexity: 2.7105\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Load fine-tuned model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained(\"./gpt2-physics\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"./gpt2-physics\")\n",
    "model.eval()\n",
    "\n",
    "# Step 1: Generation prompt\n",
    "prompt = \"[TOPIC: Electrostatics] [GRADE: 12] [LEVEL: Intermediate] [DIFFICULTY: 5]\\nQ:\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "\n",
    "# Step 2: Generate continuation\n",
    "output_ids = model.generate(\n",
    "    input_ids,\n",
    "    max_length=100,\n",
    "    do_sample=True,\n",
    "    temperature=0.9,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(\"📝 Generated Question:\\n\", generated_text)\n",
    "\n",
    "# Step 3: Evaluate loss, log-likelihood, and perplexity of generated output\n",
    "def evaluate_generated_text(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt')\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, labels=input_ids)\n",
    "        loss = outputs.loss\n",
    "        total_log_likelihood = -loss.item() * input_ids.shape[1]\n",
    "        perplexity = torch.exp(loss)\n",
    "\n",
    "    print(\"\\n📊 Evaluation Metrics:\")\n",
    "    print(f\"🔹 Token Count: {input_ids.shape[1]}\")\n",
    "    print(f\"🔹 Cross-Entropy Loss: {loss.item():.4f}\")\n",
    "    print(f\"🔹 Total Log-Likelihood: {total_log_likelihood:.4f}\")\n",
    "    print(f\"🔹 Perplexity: {perplexity.item():.4f}\")\n",
    "\n",
    "# Step 4: Evaluate\n",
    "evaluate_generated_text(generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### GPT-2 Evaluation of Gemini Created Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def compute_log_likelihood(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "        loss = outputs.loss\n",
    "        log_likelihood = -loss.item() * inputs[\"input_ids\"].shape[1]\n",
    "        avg_log_likelihood = log_likelihood / inputs[\"input_ids\"].shape[1]\n",
    "    return {\n",
    "        \"log_likelihood\": log_likelihood,\n",
    "        \"avg_log_likelihood\": avg_log_likelihood,\n",
    "        \"perplexity\": torch.exp(loss).item()\n",
    "    }\n",
    "\n",
    "gemini_mcq = \"\"\"\n",
    "Question:  A gas is compressed adiabatically from a volume of 10 L to 5 L. The initial temperature is 300 K. If the adiabatic exponent (γ) of the gas is 1.4, what is the final temperature of the gas?\n",
    "A. 390 K\n",
    "B. 424 K\n",
    "C. 458 K\n",
    "D. 492 K\n",
    "Answer: B. 424 K\n",
    "\"\"\"\n",
    "\n",
    "result = compute_log_likelihood(gemini_mcq)\n",
    "print(\"GPT-2 Evaluation of Gemini Output:\")\n",
    "print(f\"Log-Likelihood: {result['log_likelihood']:.4f}\")\n",
    "print(f\"Average Log-Likelihood per Token: {result['avg_log_likelihood']:.4f}\")\n",
    "print(f\"Perplexity: {result['perplexity']:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
