{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Please run this notebook on Google Collab to not have dependencies issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "14aR_W44Xf9N",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### GPT-2 Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "boVrVrLEUrhg",
    "outputId": "a92a8fa7-8444-49b8-c776-d8ee21073419"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/data/datasets/language_modeling.py:119: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ü§ó Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3750' max='3750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3750/3750 3:56:58, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.032400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.594400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.521300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.297500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.419700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.365700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.200700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.271000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.247800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.057000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.047400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.159200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.899300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.941600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.832800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.834400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.879200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.818400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.908100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.789400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.774400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.851300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.887800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.779000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.747400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.668000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.712200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.627500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.703600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.626500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.633100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.631100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.691100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.661700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.603200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.633700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.639800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Training complete. Model saved!\n",
      "\n",
      "üìò Generated Physics Question:\n",
      "\n",
      "[TOPIC: Electrostatics] [GRADE: 12] [LEVEL: Advanced] [DIFFICULTY: 7]\n",
      "Q: Calculate the electrostatic force between two point charges using Coulomb's Law, derived from the superposition of forces due to charges. [TYPE: Proof] [COMPLEXITY: 1.2] [COMPLEXITY: 1.1] [COMPLEXITY: 3.1] [COMPLEXITY: 2.8]\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers --quiet\n",
    "\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "from transformers import (\n",
    "    GPT2Tokenizer, GPT2LMHeadModel,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    LineByLineTextDataset,\n",
    "    Trainer, TrainingArguments\n",
    ")\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"/content/physics_questions_500.csv\")\n",
    "df.fillna(\"N/A\", inplace=True)\n",
    "\n",
    "formatted_data = []\n",
    "for _, row in df.iterrows():\n",
    "    block = f\"\"\"[TOPIC: {row['Topic']}] [GRADE: {row['grade']}] [LEVEL: {row['StudentLevel']}] [DIFFICULTY: {row['Difficulty']}] [TYPE: {row['QuestionType']}] [COMPLEXITY: {row['QuestionComplexity']}]\n",
    "[PREREQUISITES: {row['Prerequisites']}] [ESTIMATED_TIME: {row['EstimatedTime']}] [SUBJECT: {row['subject']}]\n",
    "\n",
    "Q: {row['Question']}\n",
    "A: {row['Answer']}\n",
    "\n",
    "[EXPLANATION: {row['Explanation']}]\n",
    "\"\"\"\n",
    "    formatted_data.append(block)\n",
    "\n",
    "with open(\"formatted_physics.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\\n\".join(formatted_data))\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "dataset = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"formatted_physics.txt\",\n",
    "    block_size=128\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-physics\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    save_steps=250,\n",
    "    save_total_limit=1,\n",
    "    logging_steps=100,\n",
    "    prediction_loss_only=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model(\"./gpt2-physics\")\n",
    "tokenizer.save_pretrained(\"./gpt2-physics\")\n",
    "print(\"Training complete. Model saved!\")\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"./gpt2-physics\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"./gpt2-physics\")\n",
    "model.eval()\n",
    "\n",
    "prompt = \"[TOPIC: Electrostatics] [GRADE: 12] [LEVEL: Advanced] [DIFFICULTY: 7]\\nQ:\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "\n",
    "generated = model.generate(\n",
    "    input_ids,\n",
    "    max_length=100,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    temperature=0.8,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "print(\"\\nGenerated Physics Question:\\n\")\n",
    "print(tokenizer.decode(generated[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upt2WM28XkxF"
   },
   "source": [
    "Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 486
    },
    "id": "IL6tOPJpPEiC",
    "outputId": "43ba0606-1328-47f3-f115-1af334de9be9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: gpt2-physics/ (stored 0%)\n",
      "  adding: gpt2-physics/model.safetensors (deflated 7%)\n",
      "  adding: gpt2-physics/vocab.json (deflated 68%)\n",
      "  adding: gpt2-physics/config.json (deflated 51%)\n",
      "  adding: gpt2-physics/tokenizer_config.json (deflated 56%)\n",
      "  adding: gpt2-physics/runs/ (stored 0%)\n",
      "  adding: gpt2-physics/runs/Apr12_09-28-08_e45f0af0f25d/ (stored 0%)\n",
      "  adding: gpt2-physics/runs/Apr12_09-28-08_e45f0af0f25d/events.out.tfevents.1744450089.e45f0af0f25d.240.1 (deflated 64%)\n",
      "  adding: gpt2-physics/runs/Apr12_09-23-03_e45f0af0f25d/ (stored 0%)\n",
      "  adding: gpt2-physics/runs/Apr12_09-23-03_e45f0af0f25d/events.out.tfevents.1744449787.e45f0af0f25d.240.0 (deflated 61%)\n",
      "  adding: gpt2-physics/generation_config.json (deflated 24%)\n",
      "  adding: gpt2-physics/checkpoint-3750/ (stored 0%)\n",
      "  adding: gpt2-physics/checkpoint-3750/model.safetensors (deflated 7%)\n",
      "  adding: gpt2-physics/checkpoint-3750/vocab.json (deflated 68%)\n",
      "  adding: gpt2-physics/checkpoint-3750/config.json (deflated 51%)\n",
      "  adding: gpt2-physics/checkpoint-3750/tokenizer_config.json (deflated 56%)\n",
      "  adding: gpt2-physics/checkpoint-3750/scheduler.pt (deflated 56%)\n",
      "  adding: gpt2-physics/checkpoint-3750/rng_state.pth (deflated 24%)\n",
      "  adding: gpt2-physics/checkpoint-3750/generation_config.json (deflated 24%)\n",
      "  adding: gpt2-physics/checkpoint-3750/optimizer.pt (deflated 8%)\n",
      "  adding: gpt2-physics/checkpoint-3750/merges.txt (deflated 53%)\n",
      "  adding: gpt2-physics/checkpoint-3750/training_args.bin (deflated 52%)\n",
      "  adding: gpt2-physics/checkpoint-3750/special_tokens_map.json (deflated 74%)\n",
      "  adding: gpt2-physics/checkpoint-3750/trainer_state.json (deflated 78%)\n",
      "  adding: gpt2-physics/merges.txt (deflated 53%)\n",
      "  adding: gpt2-physics/training_args.bin (deflated 52%)\n",
      "  adding: gpt2-physics/special_tokens_map.json (deflated 74%)\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_3b1a210d-daf6-447e-8897-533f708da91f\", \"gpt2-physics.zip\", 1841076737)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!zip -r gpt2-physics.zip ./gpt2-physics\n",
    "from google.colab import files\n",
    "files.download(\"gpt2-physics.zip\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QZOAbxu5XpDd"
   },
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of Gemini and GPT-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### GPt-2 Generation and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9niVzCUbS3Zb",
    "outputId": "a2e1bdfc-2d45-46fa-9f42-57a4f721b7da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìå Input Text: Q: What is the unit of electric charge? A: Coulomb.\n",
      "üîπ Token Count: 15\n",
      "üîπ Cross-Entropy Loss: 2.0931\n",
      "üîπ Total Log-Likelihood: -31.3962\n",
      "üîπ Perplexity: 8.1099\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"./gpt2-physics\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"./gpt2-physics\")\n",
    "model.eval()\n",
    "\n",
    "def get_loss_and_perplexity(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt')\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, labels=input_ids)\n",
    "        loss = outputs.loss\n",
    "        total_log_likelihood = -loss.item() * input_ids.shape[1]\n",
    "        perplexity = torch.exp(loss)\n",
    "\n",
    "    print(f\"\\n Input Text: {text}\")\n",
    "    print(f\"üîπ Token Count: {input_ids.shape[1]}\")\n",
    "    print(f\"üîπ Cross-Entropy Loss: {loss.item():.4f}\")\n",
    "    print(f\"üîπ Total Log-Likelihood: {total_log_likelihood:.4f}\")\n",
    "    print(f\"üîπ Perplexity: {perplexity.item():.4f}\")\n",
    "\n",
    "get_loss_and_perplexity(\"Q: What is the unit of electric charge? A: Coulomb.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-wcXREokXMV5",
    "outputId": "ab346c2c-71ae-4c73-eeea-cf52f3f13e9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Generated Question:\n",
      " [TOPIC: Electrostatics] [GRADE: 12] [LEVEL: Intermediate] [DIFFICULTY: 5]\n",
      "Q: What is the electric field at a point P due to two charges q1 and q2? [PERC: N/A] [SUBJECT: Physics] [TYPE: General] [COMPLEXITY: 2.6] [COMPLEXITY: 2.1] [COMPLEXITY: 1.4] [COMPLE\n",
      "\n",
      "üìä Evaluation Metrics:\n",
      "üîπ Token Count: 100\n",
      "üîπ Cross-Entropy Loss: 0.9971\n",
      "üîπ Total Log-Likelihood: -99.7149\n",
      "üîπ Perplexity: 2.7105\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Load fine-tuned model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained(\"./gpt2-physics\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"./gpt2-physics\")\n",
    "model.eval()\n",
    "\n",
    "# Step 1: Generation prompt\n",
    "prompt = \"[TOPIC: Electrostatics] [GRADE: 12] [LEVEL: Intermediate] [DIFFICULTY: 5]\\nQ:\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "\n",
    "# Step 2: Generate continuation\n",
    "output_ids = model.generate(\n",
    "    input_ids,\n",
    "    max_length=100,\n",
    "    do_sample=True,\n",
    "    temperature=0.9,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(\"üìù Generated Question:\\n\", generated_text)\n",
    "\n",
    "# Step 3: Evaluate loss, log-likelihood, and perplexity of generated output\n",
    "def evaluate_generated_text(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt')\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, labels=input_ids)\n",
    "        loss = outputs.loss\n",
    "        total_log_likelihood = -loss.item() * input_ids.shape[1]\n",
    "        perplexity = torch.exp(loss)\n",
    "\n",
    "    print(\"\\nüìä Evaluation Metrics:\")\n",
    "    print(f\"üîπ Token Count: {input_ids.shape[1]}\")\n",
    "    print(f\"üîπ Cross-Entropy Loss: {loss.item():.4f}\")\n",
    "    print(f\"üîπ Total Log-Likelihood: {total_log_likelihood:.4f}\")\n",
    "    print(f\"üîπ Perplexity: {perplexity.item():.4f}\")\n",
    "\n",
    "# Step 4: Evaluate\n",
    "evaluate_generated_text(generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### GPT-2 Evaluation of Gemini Created Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def compute_log_likelihood(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "        loss = outputs.loss\n",
    "        log_likelihood = -loss.item() * inputs[\"input_ids\"].shape[1]\n",
    "        avg_log_likelihood = log_likelihood / inputs[\"input_ids\"].shape[1]\n",
    "    return {\n",
    "        \"log_likelihood\": log_likelihood,\n",
    "        \"avg_log_likelihood\": avg_log_likelihood,\n",
    "        \"perplexity\": torch.exp(loss).item()\n",
    "    }\n",
    "\n",
    "gemini_mcq = \"\"\"\n",
    "Question:  A gas is compressed adiabatically from a volume of 10 L to 5 L. The initial temperature is 300 K. If the adiabatic exponent (Œ≥) of the gas is 1.4, what is the final temperature of the gas?\n",
    "A. 390 K\n",
    "B. 424 K\n",
    "C. 458 K\n",
    "D. 492 K\n",
    "Answer: B. 424 K\n",
    "\"\"\"\n",
    "\n",
    "result = compute_log_likelihood(gemini_mcq)\n",
    "print(\"GPT-2 Evaluation of Gemini Output:\")\n",
    "print(f\"Log-Likelihood: {result['log_likelihood']:.4f}\")\n",
    "print(f\"Average Log-Likelihood per Token: {result['avg_log_likelihood']:.4f}\")\n",
    "print(f\"Perplexity: {result['perplexity']:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
